{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d5ae82-8e05-4034-bbab-03d5fcc921d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Integrating MindBridge with Databricks: A Step-by-Step Guide\n",
    "This tutorial will guide you through the process of integrating MindBridge with Databricks. We will walkthrough how to set up an Organization and Engagement using the MindBridge SDK, how to perform a General Ledger Analysis on data from Databricks, then how you can extract key information from your Analysis Results. Here's what we'll cover:\n",
    "\n",
    "1. **Installing the MindBridge SDK in Databricks**  \n",
    "   We'll start by installing the MindBridge SDK in your Databricks environment, enabling access to MindBridge's API features within your notebooks.\n",
    "\n",
    "2. **Storing your MindBridge API Token in Databricks**  \n",
    "   Learn to securely save your MindBridge API token in a Databricks-backed secret scope. This step ensures that sensitive credentials are safely managed within Databricks.\n",
    "\n",
    "3. **Loading your MindBridge API Token in Databricks**  \n",
    "   With the token stored, we'll show you how to load and use it to configure your API connection.\n",
    "\n",
    "4. **Setting Up an Organization and Engagement**  \n",
    "   Here, you'll learn how to configure an Organization and Engagement for your Analysis.\n",
    "\n",
    "5. **Uploading Files to the File Manager**  \n",
    "   How to upload files into the File Manager created for your Engagement\n",
    "\n",
    "6. **Creating and Running Analyses**  \n",
    "   In this section, you'll learn how to create a new analysis, link the necessary data from the File Manager, and execute the analysis.\n",
    "\n",
    "7. **Getting the Analysis Results**  \n",
    "   Finally, we'll retrieve and display the results of your analysis, providing insights into your data that you can further leverage within Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a221f5c1-5e69-4dfd-a142-38b09d420edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installing the MindBridge SDK in Databricks\n",
    "The following commands can be run to install the [mindbridge-api-python-client](https://pypi.org/project/mindbridge-api-python-client/) to your currently selected cluster. Version 1.5.1 or newer is required for the steps in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c1d39c-34f8-44d2-8d79-89c486725ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade mindbridge-api-python-client pydantic<2.12\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407a087f-571e-4acf-94f8-1e38711de33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analysis_url = \"https://psus.mindbridge.ai/app/organization/68efe134a3c10e74eb3d6b81/engagement/68eff222067ffa4a9dc98640/analysis/68eff47f067ffa4a9dc98749/analyze/summary?productCode=VENDOR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fd13672-8bc9-45ae-a5d1-94d985e4a0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Storing Your MindBridge API Token in Databricks\n",
    "If you do not have an API Token already you can follow [Create an API token](https://support.mindbridge.ai/hc/en-us/articles/9349943782039-Create-an-API-token) on our knowledge base. Once that is done, we'll [Create a Databricks-backed secret scope](https://learn.microsoft.com/en-us/azure/databricks/security/secrets/#create-a-databricks-backed-secret-scope) using the Databricks CLI to securely store your token. If you haven't set up the Databricks CLI yet, you can follow [Install or update the Databricks CLI](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install).\n",
    "\n",
    "After installing and configuring the CLI, use the following commands to create a new secret scope and add your API Token to the scope:\n",
    "\n",
    "```sh\n",
    "# Create the secret scope named \"mindbridge-api-tutorials\"\n",
    "databricks secrets create-scope mindbridge-api-tutorials\n",
    "\n",
    "# Add your MindBridge API Token to the scope\n",
    "databricks secrets put-secret mindbridge-api-tutorials MINDBRIDGE_API_TOKEN\n",
    "```\n",
    "\n",
    "Once your token is added, you can verify its existence by running the following:\n",
    "```sh\n",
    "databricks secrets list-scopes\n",
    "databricks secrets list-secrets mindbridge-api-tutorials\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401879bc-632c-4bf6-a47c-e156a4e540d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib3.util import parse_url\n",
    "\n",
    "parsed_url = parse_url(analysis_url)\n",
    "mindbridge_url = parsed_url.host\n",
    "\n",
    "parsed_url_path = parsed_url.path.split(\"/\")\n",
    "analysis_result_id = parsed_url_path[parsed_url_path.index('analysis') + 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109a403a-ae91-4d2d-a0f7-89f9434dfacc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading your MindBridge API Token in Databricks\n",
    "\n",
    "In this section, we'll load the MindBridge API token securely stored in Databricks and configure the API connection. Replace the ```url``` with the url for your MindBridge instance. Upon execution, you should see details about the user associated with your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392f5983-f50e-4cf0-a9cd-99bec71e940e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mindbridgeapi as mbapi\n",
    "\n",
    "# Load your token from the secret scope\n",
    "token = dbutils.secrets.get(\n",
    "    scope=\"mindbridge-api-tutorials\", key=\"MINDBRIDGE_API_TOKEN\"\n",
    ")\n",
    "\n",
    "# Create a connection to the server\n",
    "server = mbapi.Server(url=mindbridge_url, token=token)\n",
    "\n",
    "# Get the current user\n",
    "analysis_result = server.analysis_results.get_by_id(analysis_result_id)\n",
    "analysis = server.analyses.get_by_id(analysis_result.analysis_id)\n",
    "print(analysis.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bc0a82-a948-4498-b1fa-7ce92c3ba7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setting up an Organization and Engagement\n",
    "In this section, we'll walk through setting up an Organization and an Engagement to house your Analysis. We'll set up the Engagement using the MindBridge for-profit library. If you already have an existing Organization and Engagement you would like to use, you can update the `organization_name` and `engagement_name` values to use your preferred entities. If your Engagement uses a different Library, you may need to update later steps so that your Analysis Type and Analysis Source Types are compatible with the Library you selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0a3f90-ad6b-40cc-8062-df86a53bb47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define details for organization, engagement, and library\n",
    "organization_name = \"MindBridge Databricks Integration\"\n",
    "engagement_name = \"My Engagement\"\n",
    "library_name = \"MindBridge for-profit\"\n",
    "\n",
    "# Step 1: Create the Organization, or get it if it already exists\n",
    "try:\n",
    "    organization_item = mbapi.OrganizationItem(\n",
    "        name=organization_name,\n",
    "        external_client_code=\"My Client ID\",  # Optional\n",
    "        manager_user_ids=[user.id],  # Optional\n",
    "    )\n",
    "    organization = server.organizations.create(organization_item)\n",
    "    print(f\"Created the Organization: '{organization_name}'\")\n",
    "except mbapi.exceptions.ValidationError:\n",
    "    organization = next(server.organizations.get({\"name\": organization_name}))\n",
    "    print(f\"Organization '{organization_name}' already exists. Fetched it instead.\")\n",
    "\n",
    "# Step 2: Get the Library we want to use in the Engagement\n",
    "system_libraries = server.libraries.get({\"system\": True})\n",
    "mindbridge_for_profit_library = next(\n",
    "    (x for x in system_libraries if x.name == library_name), None\n",
    ")\n",
    "\n",
    "# Step 3: Create the Engagement, or get it if it already exists\n",
    "try:\n",
    "    engagement_item = mbapi.EngagementItem(\n",
    "        organization_id=organization.id,\n",
    "        name=engagement_name,\n",
    "        engagement_lead_id=user.id,\n",
    "        library_id=mindbridge_for_profit_library.id,\n",
    "    )\n",
    "    engagement = server.engagements.create(engagement_item)\n",
    "    print(f\"Created the Engagement: '{engagement_name}'\")\n",
    "except mbapi.exceptions.ValidationError:\n",
    "    engagement = next(\n",
    "        server.engagements.get(\n",
    "            {\"organizationId\": organization.id, \"name\": engagement_name}\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"Engagement '{engagement_name}' already exists within the Organization. \"\n",
    "        \"Fetched it instead.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17a3ad3b-945f-49ed-ae5c-6b8f0ba92ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uploading Files to the File Manager\n",
    "After an Engagement is created, a File Manager entity is automatically set up to store data used by Analyses within the Engagement. We will upload our example General Ledger file from the data folder to the File Manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd516509-e1f3-4422-a0a6-b2d44006480c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "relative_path = \"./data/GENERAL_LEDGER_JOURNAL.csv\"\n",
    "full_path = Path(relative_path).resolve()\n",
    "\n",
    "gl_file_manager_item = mbapi.FileManagerItem(engagement_id=engagement.id)\n",
    "\n",
    "# Upload General Ledger\n",
    "gl_file_manager_file = server.file_manager.upload(\n",
    "    input_item=gl_file_manager_item, input_file=full_path\n",
    ")\n",
    "\n",
    "# We can check to see if our files are in the File Manager\n",
    "file_manager_generator = server.file_manager.get({\"engagementId\": engagement.id})\n",
    "print(\"Here are the files in the File Manager for your Engagement\")\n",
    "for file_manager_entity in file_manager_generator:\n",
    "    print(file_manager_entity.original_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab51eed-6912-4898-946d-686b60faa41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating and Running Analyses\n",
    "With our Engagement set up and files uploaded, we're ready to create an Analysis. We'll import the General Ledger File Manager as an Analysis Source, mark the Account Mappings as verified then run the Analysis.\n",
    "\n",
    "If you used a custom library instead of the MindBridge for-profit library used in *Setting up an Organization and Engagement*, you will need to upload a Chart of Accounts or use the Account Mappings endpoints to map the accounts within the General Ledger. In this example we will use the Verify Mappings endpoint to use the suggested Account Mappings created by MindBridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1a4fec-e1f6-4e94-b9a4-923eb853fc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create an analysis\n",
    "analysis_item = mbapi.AnalysisItem(\n",
    "    engagement_id=engagement.id,\n",
    "    analysis_periods=[{\"startDate\": \"2020-01-01\", \"endDate\": \"2021-01-01\"}],\n",
    "    analysis_type_id=mbapi.AnalysisTypeItem.GENERAL_LEDGER,\n",
    "    currency_code=\"CAD\",\n",
    "    name=\"General Ledger Analysis\",\n",
    ")\n",
    "\n",
    "analysis = server.analyses.create(analysis_item)\n",
    "\n",
    "# Add the General Ledger from the File Manager as an Analysis Source\n",
    "gl_analysis_source_item = mbapi.AnalysisSourceItem(\n",
    "    engagement_id=engagement.id,\n",
    "    analysis_id=analysis.id,\n",
    "    file_manager_file_id=gl_file_manager_file.id,\n",
    "    analysis_period_id=analysis.analysis_periods[0].id,\n",
    "    analysis_source_type_id=mbapi.AnalysisSourceTypeItem.GENERAL_LEDGER_JOURNAL,\n",
    "    target_workflow_state=mbapi.TargetWorkflowState.COMPLETED,\n",
    ")\n",
    "\n",
    "print(\"Creating Analysis Source\")\n",
    "gl_analysis_source = server.analysis_sources.create(gl_analysis_source_item)\n",
    "\n",
    "print(\"Waiting for Analysis Source to be ready\")\n",
    "max_polls = 5 * 60\n",
    "polls = 0\n",
    "prev_state = \"\"\n",
    "while polls < max_polls:\n",
    "    polls += 1\n",
    "    time.sleep(1)\n",
    "    gl_analysis_source = server.analysis_sources.get_by_id(gl_analysis_source.id)\n",
    "    if gl_analysis_source.workflow_state.value != prev_state:\n",
    "        print(f\"Current State: {gl_analysis_source.workflow_state.value}\")\n",
    "        prev_state = gl_analysis_source.workflow_state.value\n",
    "    if (\n",
    "        gl_analysis_source.workflow_state.value\n",
    "        == gl_analysis_source_item.target_workflow_state.value\n",
    "    ):\n",
    "        print(\"Analysis Source is ready.\")\n",
    "        break\n",
    "\n",
    "print(\"Setting Account Mappings to Verified\")\n",
    "engagement = server.engagements.verify_account_mappings(engagement)\n",
    "analysis = server.analyses.wait_for_analysis_sources(analysis)\n",
    "\n",
    "print(\"Running the Analysis\")\n",
    "analysis = server.analyses.run(analysis)\n",
    "print(\"Analysis has been started\")\n",
    "\n",
    "analysis = server.analyses.wait_for_analysis(analysis)\n",
    "print(\"Analysis is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f957f9af-7ef6-4779-bad8-c04a53f04c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting the Analysis Results\n",
    "\n",
    "### Overview of Data Table information\n",
    "First we will take a look at what Data Tables exist on this Analysis and how the data is structured. Below is some code to generate an overview of each Data Table in the Analysis, including the fields, data type and whether the field is searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5260c5b0-81f4-4767-886b-04c81516e4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "server.analyses.restart_data_tables(analysis)\n",
    "\n",
    "# Loop through each data_table and display information\n",
    "for data_table in analysis.data_tables:\n",
    "    # Display general data_table information\n",
    "    general_info = pd.DataFrame(\n",
    "        {\n",
    "            \"Analysis ID\": [data_table.analysis_id],\n",
    "            \"Data Table ID\": [data_table.id],\n",
    "            \"Logical Name\": [data_table.logical_name],\n",
    "            \"Type\": [data_table.type],\n",
    "        }\n",
    "    )\n",
    "    display(general_info)\n",
    "\n",
    "    # Prepare column data for this data_table\n",
    "    column_data = [\n",
    "        {\n",
    "            \"Field\": col.field,\n",
    "            \"Filter Only\": col.filter_only,\n",
    "            \"Keyword Search\": col.keyword_search,\n",
    "            \"Type\": col.type.value,\n",
    "        }\n",
    "        for col in data_table.columns\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame for column data and display it\n",
    "    column_df = pd.DataFrame(column_data)\n",
    "    display(column_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fcbcdc-9b26-49ff-8396-5b7d81ea4a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extracting Key Info from MindBridge \n",
    "Now let's extract specific data from our Data Tables. In this example we will fetch all of the transactions which had a Risk Score greater or equal to 30%. Then we will save the results in a CSV and load them into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c53e883-a42f-4816-a79b-1eb2769695de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the output file path where the results will be saved\n",
    "\n",
    "output_relative_path = \"./results/result.csv\"\n",
    "output_full_path = Path(output_relative_path).resolve()\n",
    "\n",
    "print(\"Requesting Elevated Risk General Ledger Transactions\")\n",
    "# Restart data tables to reset generator\n",
    "server.analyses.restart_data_tables(analysis)\n",
    "\n",
    "# Select the transactions table\n",
    "data_table = next(x for x in analysis.data_tables if x.logical_name == \"gl_journal_tx\")\n",
    "\n",
    "# Define a query to extract transactions with a risk score greater than or equal to 30%\n",
    "query = {\"risk\": {\"$gte\": 3000}}\n",
    "\n",
    "# Export the filtered data\n",
    "export_async_result = server.data_tables.export(data_table, query=query)\n",
    "server.data_tables.wait_for_export(export_async_result)\n",
    "\n",
    "# Define the output path for saving the CSV file\n",
    "path_output = server.data_tables.download(\n",
    "    export_async_result, output_file_path=output_full_path\n",
    ")\n",
    "print(f\"Success! Saved to: {path_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8091d9a9-9d70-48ab-b5fc-2376af74d956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file saved in the previous step in pandas\n",
    "\n",
    "df = pd.read_csv(output_full_path)\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 616437765715387,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "7_Task_Assignment_Prototype",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
